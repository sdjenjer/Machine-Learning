% \documentclass{amsart}[12pt]
% \textwidth=16.2cm\textheight=23.5cm\hoffset=-1.2cm\voffset=-1.5cm
\documentclass[a4paper, 12pt]{amsart}

\usepackage[textwidth=16.2cm, textheight=23.5cm, left=1.5cm, right=1.5cm]{geometry}

\input{rus}
\input{packages}
\input{preamble}
\input{listmain}


\theoremstyle{plain}
    \newtheorem*{state}{Задача}
    \newtheorem{assertion}{Предложение}

\newcommand{\mytitle}{Конспект по линейной регрессии}
\title[\mytitle]{\mytitle}
\author{Дженжер Святослав}
\date{декабрь 2024}

\begin{document}

\maketitle

\section{Конспект темы}

Обе линейные модели описываются следующим образом.
На пространстве $X$ объектов заданы числовые признаки \( f_1, \ldots, f_n\colon X\to\R\). По заданному вектору \(w\in\R^n\) весов определяются предсказания объектов
\[
    a(x,w) := \sum_{i=1}^n w_if_i(x) = w^Tf(x)
\]
в задаче регрессии; в задаче классификации от этого выражения берётся знак. При фиксированной (но неизвестной) таргет-функции \( y\colon X\to Y\) определяется функция потерь \( L(x, w)\), которая в задаче классификации равна индикатору того, что \(a(x,w) \neq y(x)\), а в задаче регресси равна
\[
    L(x,w) = \abs{a(x,w)-y(x)}
    \quad\text{или}\quad
    L(x,w) = \abs{a(x,w)-y(x)}^2,
\]
или любому другому симметричному неотрицательному функционалу.
Далее в обеих моделях нужно решить задачу оптимизации
\[
    \sum_{i=1}^N L(x_i, w) \to \min_{w},
\]
где $x_i$ суть элементы выборки размера $N$. То есть, неформально, нужно придумать линейную модель, которая по $x \in X$ восстанавливает $y(x) \in Y$.

В методе наименьших квадратов в задаче линейной регресии необходимо решить задачу минимизации функционала
\[
    \norm{w^Tf(X) - Y}_2^2 \to \min_w,
\]
где $Y$ это вектор из таргетов $y_i = y(X_i)$, $X$ это вектор наблюдений, а $f(X)$ это матрица признаков размера $n \times N$.
Из линейной алгебры известно, что решением задачи минимизации является вектор
\[
    \hat{w} = (f(X)f(X)^T)^{-1}f(X)Y.
\]
Известно, что он является несмещённой оценкой параметра $w$, и, кроме того, наилучшей оценкой этого параметра в классе всех линейных оценок вида $A(X)Y$ с квадратичной функцией потерь.

В гауссовской линейной модели дополнительно известно, что
\[
    Y \sim \Normald\left( w^Tf(X),\ \sigma^2 I_{N\times N} \right).
\]
Тогда оценка $\hat{w}$ является оптимальной оценкой вектора $w$ в среднеквадратичном подходе в классе несмещённых оценок.

\section*{Задачи по теме}

\begin{state}
    Дан набор точек в $\R^3$, отмеченных $\pm1$.
    Необходимо описать задачу проведения разделяющей плоскости, проходящей через начало координат, как задачу линейной регрессии.
\end{state}

\begin{state}
    В задаче линейной регрессии получить несмещённую оценку ошибки $\sigma^2$.
\end{state}

\begin{state}
    В гауссовской линейной модели найдите оптимальную оценку параметра $w_1 + \ldots +w_n$ и её распределение.
\end{state}

\end{document}